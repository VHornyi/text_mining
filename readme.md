# Lord of the Rings - Text Mining Project

This project performs various text mining analyses on the dialogue scripts from the three books of "The Lord of the Rings" trilogy: "The Fellowship of the Ring," "The Two Towers," and "The Return of the King." The primary goal is to demonstrate various text processing and analysis techniques.

## Project Structure

The project directory (`TEXT_MINING/`) is organized as follows:

*   `requirements.txt`        - Python package dependencies.
*   `readme.md`               - This documentation file.
*   `text_mining/`            - Main package containing the source code and data.
    *   `__init__.py`         - (Optional, makes `text_mining` a package)
    *   `main.py`             - The main executable script to run the entire analysis pipeline.
    *   `utils.py`            - Contains utility functions for data loading and text preprocessing.
    *   `analysis.py`         - Contains functions for performing various text analyses (frequency, sentiment, topics, etc.).
    *   `data/`               - Directory for input data files.
        *   `01 Fellowship.txt`
        *   `02 Two Towers.txt`
        *   `03 Return of the King.txt`
        *   `races.csv`         - (Character race information, currently not fully utilized).
    *   `__pycache__/`        - (Auto-generated Python cache directory).
*   `results/`                - (Suggested directory to store generated plots if saving is implemented. Plots are currently shown interactively).
    *   `document_clusters.png` 
    *   `dtm.png`               
    *   `Overall_top_25.png`    
    *   `sentiment.png`         
    *   `tf-idf.png`            
    *   `word_cloud.png`        
*   `__pycache__/`           

*(Note: The `results/` directory and automatically saving plots to it are not explicitly implemented in the current scripts. Plots are displayed interactively during script execution. 
## Technical Setup

### Prerequisites
*   Python 3.9+ (tested with Python 3.12)
*   pip (Python package installer)

### Installation
1.  **Clone the repository (or download the files):**
    If you are using Git:
    ```bash
    git clone <repository_url>
    cd TEXT_MINING
    ```
    Otherwise, download and extract the project files into a directory named `TEXT_MINING`.

2.  **Install required Python packages:**
    Navigate to the root directory of the project (`TEXT_MINING/`) where `requirements.txt` is located and run:
    ```bash
    # Replace 'python' with your specific python executable if needed
    # (e.g., python3, or the full path to your python.exe)
    python -m pip install -r requirements.txt
    ```
    This will install `pandas`, `nltk`, `scikit-learn`, `matplotlib`, `wordcloud`, `gensim`, and `seaborn`.

3.  **NLTK Data Download:**
    The first time you run the script, it will attempt to download necessary NLTK resources (`wordnet`, `punkt`, `stopwords`, `omw-1.4`, `vader_lexicon`). Ensure you have an internet connection.

### Running the Analysis
To execute the text mining pipeline, run the `main.py` script from the `text_mining` sub-directory:
```bash
# Navigate to the text_mining sub-directory
cd text_mining

# Run the main script
# Replace 'python' with your specific python executable if needed
python main.py


Alternatively, from the root TEXT_MINING directory:

# Replace 'python' with your specific python executable if needed
python text_mining/main.py
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Bash
IGNORE_WHEN_COPYING_END

Plots generated by the analysis will be displayed in separate windows during execution.

Data Encoding

The script is currently configured to read the input .txt files using cp1252 encoding (see utils.py, load_lotr_texts() function). This was determined to work for the provided sample files. If you use different text files and encounter encoding errors:

Try changing the encoding parameter in the open() function within utils.py. Common alternatives include utf-8 or latin1.

Alternatively, convert your .txt files to cp1252 or UTF-8 encoding using a text editor (e.g., VS Code, Notepad++).

Analysis Pipeline

The main.py script orchestrates the following text mining steps:

Data Loading (utils.py):

Loads dialogue texts from "01 Fellowship.txt", "02 Two Towers.txt", and "03 Return of the King.txt".

Combines all texts into a single corpus for global analysis and keeps individual book texts for per-book analysis.

Text Preprocessing (utils.py):

Converts text to lowercase.

Removes punctuation and digits using regular expressions.

Tokenizes text into words.

Removes common English stop words.

Filters out very short tokens (less than 3 characters).

Applies lemmatization to reduce words to their base form (e.g., "running" -> "run").

Basic Text Analysis Methods (analysis.py):

Word Frequency Analysis:

Calculates and plots the top 25 most frequent words in the combined corpus.

Generates a word cloud from the combined corpus.

N-gram Analysis:

Identifies and prints the top 15 most frequent bigrams (2-word sequences).

Identifies and prints the top 10 most frequent trigrams (3-word sequences).

DTM and TF-IDF Matrix Construction:

Builds a Document-Term Matrix (DTM) using CountVectorizer (term frequencies).

Builds a TF-IDF Matrix using TfidfVectorizer (term importance).

Both are limited to MAX_FEATURES_FOR_VECTORIZERS (currently 50) for manageability and heatmap visualization.

Generates and displays heatmaps for both DTM and TF-IDF matrices to visualize term distributions across books.

Unsupervised Machine Learning (analysis.py):

Sentiment Analysis (VADER):

Calculates the compound sentiment score for each book using VADER.

Prints the scores and classifies sentiment as POSITIVE, NEGATIVE, or NEUTRAL.

Displays a bar chart visualizing the sentiment scores for each book.

Topic Modeling:

Latent Dirichlet Allocation (LDA): Applied to the DTM to discover abstract topics. The number of topics is dynamically adjusted (max 3, or fewer if not enough documents). Prints top words for each topic.

Latent Semantic Analysis (LSA/SVD): Applied to the TF-IDF matrix. The number of topics is also dynamically adjusted. Prints top words for each topic.

Document Clustering (K-Means):

Clusters the books (documents) based on their TF-IDF representations using K-Means.

The number of clusters is dynamically set (max 3, or fewer if not enough documents).

Prints cluster assignments for each book.

Visualizes the clusters in 2D space using PCA for dimensionality reduction.

Word Embeddings (Word2Vec):

Trains a Word2Vec model on the processed tokens from all books (sentences are lists of tokens per book).

Finds and prints words most similar to a target word (default: "frodo").

Attempts a word analogy task (e.g., "king - man + woman = ?").

Interpreting Results

Frequency and N-grams: Reveal key characters, items, and common phrases.

DTM/TF-IDF Heatmaps: Show which of the top terms are prominent in each book.

Sentiment Analysis: Provides an overall emotional tone for each book based on its vocabulary.

Topic Models: Help identify underlying themes or subjects discussed across the texts. The quality of topics can be influenced by the small corpus size (3 documents) and the reduced number of features.

Clustering: Shows how textually similar or different the books are.

Word Embeddings: Demonstrate semantic relationships learned from the text. Similarity scores and analogy results are highly dependent on the corpus size and content.

Limitations and Potential Improvements

Small Corpus: Analysis is based on only three documents (dialogue scripts). This can impact the robustness of statistical methods like topic modeling and word embeddings.

Dialogue Only: The analysis uses dialogue scripts, not the full narrative text, which might offer richer insights.

Feature Reduction: Limiting max_features for DTM/TF-IDF to 50 for heatmap visualization might cause some information loss for downstream tasks like topic modeling.

Simple Preprocessing: While standard, more advanced techniques (e.g., custom stop-word lists, Part-of-Speech tagging based filtering) could be explored.

VADER's Limitations: VADER is lexicon-based and may not capture complex nuances, sarcasm, or overall narrative sentiment accurately.

Future Work:

Incorporate character-specific analysis using races.csv.

Implement functions for loading text from other formats (PDF, Word, web pages).

Add stemming as an alternative to lemmatization.

Explicitly calculate and display word similarity measures (e.g., cosine similarity between selected word vectors from Word2Vec).

Implement binary DTM construction.

Explore other visualization types (e.g., association plots, network graphs).

Use more advanced sentiment analysis models.


